# Unique name of Kubernetes cluster. In order to deploy
# more than one cluster into the same AWS account, this
# name must not conflict with an existing cluster.
clusterName: {{ wrapped.cluster_name }}

# CoreOS release channel to use. Currently supported options: alpha, beta, stable
# See coreos.com/releases for more information
#releaseChannel: stable

# The AMI ID of CoreOS.
# If omitted, the latest AMI for the releaseChannel is used.
#amiId: ""
# The ID of hosted zone to add the externalDNSName to.
# Either specify hostedZoneId or hostedZone, but not both
#hostedZoneId: ""

# Network ranges of sources you'd like SSH accesses to be allowed from, in CIDR notation. Defaults to ["0.0.0.0/0"] which allows any sources.
# Explicitly set to an empty array to completely disable it.
# If you do that, probably you would like to set securityGroupIds to provide this worker node pool an existing SG with a SSH access allowed from specific ranges.
sshAccessAllowedSourceCIDRs:

# The name of one of API endpoints defined in `apiEndpoints` below to be written in kubeconfig and then used by admins
# to access k8s API from their laptops, CI servers, or etc.
# Required if there are 2 or more API endpoints defined in `apiEndpoints`
#adminAPIEndpointName: versionedPublic

# Kubernetes API endpoints with each one has a DNS name and is with/without a managed/unmanaged ELB, Route 53 record set
# CAUTION: `externalDNSName` must be omitted when there are one or more items under `apiEndpoints`
apiEndpoints:
- # The unique name of this API endpoint used to identify it inside CloudFormation stacks or
  # to be referenced from other parts of cluster.yaml
  name: default

  # DNS name for this endpoint, added to the kube-apiserver TLS cert
  # It must be somehow routable to the Kubernetes controller nodes
  # from worker nodes and external clients. Configure the options
  # below if you'd like kube-aws to create a Route53 record sets/hosted zones
  # for you.  Otherwise the deployer is responsible for making this name routable
  dnsName: {{ wrapped.api_dns_name }}

  # Configuration for an ELB serving this endpoint
  # Omit all the settings when you want kube-aws not to provision an ELB for you
  loadBalancer:
    # Specifies an existing load-balancer used for load-balancing controller nodes and serving this endpoint
    # Setting id requires all the other settings excluding `name` to be omitted because reusing an ELB implies that configuring other resources
    # like a Route 53 record set for the endpoint is now your responsibility!
    # Also, don't forget to add controller.securityGroupIds to include a glue SG to allow your existing ELB to access controller nodes created by kube-aws
    #id: existing-elb

    # Set to true when you want kube-aws to create a Route53 ALIAS record set for this API load balancer for you
    # Must be omitted when `id` is specified
    createRecordSet: false
    # All the subnets assigned to this load-balancer. Specified only when this load balancer is not reused but managed one
    # Must be omitted when `id` is specified
    subnets:
    #- name: managedPublic1
    - name: ExistingPublicSubnet1
    - name: ExistingPublicSubnet2
    - name: ExistingPublicSubnet3

    # Set to true so that the managed ELB becomes an `internal` one rather than `internet-facing` one
    # When set to true while subnets are omitted, one or more private subnets in the top-level `subnets` must exist
    # Must be omitted when `id` is specified
    private: false

    # TTL in seconds for the Route53 RecordSet created if createRecordSet is set to true.
    #recordSetTTL: 300

    # The Route 53 hosted zone is where the resulting Alias record is created for this endpoint
    # Must be omitted when `id` is specified
    #hostedZone:
    #  # The ID of hosted zone to add the dnsName to.
    #  id: ""
#    # Network ranges of sources you'd like Kubernetes API accesses to be allowed from, in CIDR notation. Defaults to ["0.0.0.0/0"] which allows any sources.
#    # Explicitly set to an empty array to completely disable it.
#    # If you do that, probably you would like to set securityGroupIds to provide this load balancer an existing SG with a Kubernetes API access allowed from specific ranges.
    apiAccessAllowedSourceCIDRs:
    # todo [customize] per account & region
    {% for nat_gw in wrapped.nat_gw_ips %}
    - {{ nat_gw.ip_address }}
    {% endfor %}

#    # Existing security groups attached to this load balancer which are typically used to
#    # allow Kubernetes API accesses from admins and/or CD systems when `apiAccessAllowedSourceCIDRs` are explicitly set to an empty array
    # todo [customize] per account & region
    # The group id of aws-composer-custom-k8s-api-internal-access-sg*
    securityGroupIds:
    - {{ wrapped.api_private_access_sg }}

#
#  #
#  # Common configuration #1: Unversioned, internet-facing API endpoint
#  #
#  - name: unversionedPublic
#    dnsName: api.example.com
#    loadBalancer:
#      id: elb-abcdefg
#
#  #
#  # Common configuration #2: Versioned, internet-facing API endpoint
#  #
#  - name: versionedPublic
#    dnsName: v1api.example.com
#    loadBalancer:
#      hostedZone:
#        id: hostedzone-abcedfg
#
#  #
#  # Common configuration #3: Unmanaged endpoint a.k.a Extra DNS name added to the kube-apiserver TLS cert
#  #
#  - name: extra
#    dnsName: youralias.example.com

# Name of the SSH keypair already loaded into the AWS
# account being used to deploy this cluster.
# todo [customize] per stack
keyName: {{ wrapped.debug_key_name }}

# Additional keys to preload on the coreos account (keep this to a minimum)
#sshAuthorizedKeys:
# - "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDPZPLtRvmeD8ZR/cm3SQ2GXc/tpDGPwP0slQDq3OV9R8+QiArvvfaGPpALM4xf3Y+DQxkG5W+PzPpYGUy4B17HVB9dywSZEjroqvf5/3LZp+X8dVQ/NPnOcox31zkmKhh8zODeWgcOv+tmf6hG82oGlMZci/6NMajpUT2RZYJfhN/aX+zeKkVZr1rckhuva7p0k3mcbtom9dMK0+W9rs2TjRTCrKo0l4jl9hhlc0omfRyQhvFmji/pgKoUoST9GFP7vS21QIAYR6K5PcIjYwn+p/jPKATHCD5PKHDilu+wt7GUaf3VyXljKavJsSAOydso+qsYivj/uwWhYfxppMTR sorin.buliarca@ft.com"

# Region to provision Kubernetes cluster
# todo [customize] per region
region: {{ wrapped.region }}

# Availability Zone to provision Kubernetes cluster when placing nodes in a single availability zone (not highly-available) Comment out for multi availability zone setting and use the below `subnets` section instead.
#availabilityZone: eu-west-1a

# ARN of the KMS key used to encrypt TLS assets.
# todo [customize] per account & region
kmsKeyArn: "{{ wrapped.kms_key_arn }}"

controller:
#  # Number of controller nodes to create, for more control use `controller.autoScalingGroup` and do not use this setting
  count: {{ wrapped.controller_count }}
#
#  # Maximum time to wait for controller creation
#  createTimeout: PT15M
#
#  # Instance type for controller node.
#  # CAUTION: Don't use t2.micro or the cluster won't work. See https://github.com/kubernetes/kubernetes/issues/18975
  instanceType: {{ wrapped.controller_instance_type }}
#
#  rootVolume:
#    # Disk size (GiB) for controller node
#    size: 30
#    # Disk type for controller node (one of standard, io1, or gp2)
#    type: gp2
#    # Number of I/O operations per second (IOPS) that the controller node disk supports. Leave blank if controllerRootVolumeType is not io1
#    iops: 0
#
#  # Existing security groups attached to controller nodes which are typically used to
#  # (1) allow access from controller nodes to services running on an existing infrastructure
#  # (2) allow ssh accesses from bastions when `sshAccessAllowedSourceCIDRs` are explicitly set to an empty array
  securityGroupIds:
    # allow SSH from internal VPC machines. Group name "Internal SSH security group"
    # todo [customize] per account & region
    - {{ wrapped.vpc_internal_ssh_sg }}

#
#  # Auto Scaling Group definition for controllers. If only `controllerCount` is specified, min and max will be the set to that value and `rollingUpdateMinInstancesInService` will be one less.
#  autoScalingGroup:
#    minSize: 1
#    maxSize: 2
#    rollingUpdateMinInstancesInService: 1
#  # If you specify managedIamRoleName the role created for controller nodes will not suffix the random id at the end
#  # Role will be created with "Ref": {"AWS::StackName"}-{"AWS::Region"}-yourManagedRole
#  # to follow the recommendation in AWS documentation  http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html
#  # This is also intended to be used in combination with .experimental.kube2IamSupport. See #297 for more information.
#  #
#  # ATTENTION: Consider limiting number of characters in clusterName and iam.role.name to avoid the resulting IAM
#  # role name's length from exceeding the AWS limit: 64
#  # See https://github.com/kubernetes-incubator/kube-aws/issues/347
#  # if you omit this block kube-aws would create an IAM Role and a customer managed policy with a random name.
  iam:
#    role:
#       # This will create a named IAM Role managed by kube-aws with the name {AWS::Region}-$managedIamRoleName.
#       # It will have attached a customer Managed Policy that you can modify afterwards if you need more permissions for your cluster.
#       # Be careful with the Statements you modify because an update could overwrite your own.
#       # the Statements included in the ManagedPolicy are the minimun ones required for the Controllers to run.
#       # name: "yourManagedRole"
#       # if you set managedPolicies here it will be attached in addition to the created managedPolicy in kube-aws for the cluster.
#       # CAUTION: if you attach a more restrictive policy in some resources (i.e ec2:* Deny) you can make kube-aws fail.
#       # managedPolicies:
#       # -  arn: "arn:aws:iam::aws:policy/AdministratorAccess"
#       # -  arn: "arn:aws:iam::YOURACCOUNTID:policy/YOURPOLICYNAME"

#       # if you set an InstanceProfile kube-aws will NOT create any IAM Role and will use the configured instanceProfile.
#       # CAUTION: you must ensure that the IAM Role linked to the listed InstanceProfile has enough permissions to ensure kube-aws to run.
#       # if you dont know which permissions are required is recommended to create a cluster with a managed role.
    instanceProfile:
      # todo [customize] per account & region
      arn: "{{ wrapped.controller_iam_role }}"


#  # If omitted, public subnets are created by kube-aws and used for controller nodes
  subnets:
#    # References subnets defined under the top-level `subnets` key by their names
    - name: ExistingPrivateSubnet1
    - name: ExistingPrivateSubnet2
    - name: ExistingPrivateSubnet3

#
#   # Kubernetes node labels to be added to controller nodes
  nodeLabels:
    kube-aws.coreos.com/role: controller
#
#  # User defined files that will be added to the Controller cluster cloud-init configuration in the "write_files:" section.
  customFiles:
  - path: /etc/systemd/journald.conf.d/10-override-config.conf
    permissions: 0644
    content: |
      [Journal]
      MaxLevelConsole=crit
      Compress=false
      RateLimitInterval=0
      RateLimitBurst=0

  - path: /etc/systemd/system/docker.service.d/99-docker-opts-override.conf
    permissions: 0644
    content: |
      [Service]
      Environment="{% raw %}DOCKER_OPTS=--log-driver=journald --host 0.0.0.0:2375 --log-opt tag={{.ImageName}}{% endraw %}"

#    - path: "/etc/rkt/auth.d/docker.json"
#      permissions: 0600
#      content: |
#        { "rktKind": "dockerAuth", "rktVersion": "v1", ... }
#
#  # User defined systemd units that will be added to the Controller cluster cloud-init configuration in the "units:" section.
  customSystemdUnits:
  - name: systemd-journald.service
    command: restart
  - name: authorized_keys.service
    command: start
    content: |
        [Unit]
        Description=Update authorized_keys

        [Service]
        Type=oneshot
        ExecStartPre=/bin/sh -c "mkdir -p /home/core/.ssh && touch /home/core/.ssh/authorized_keys"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys.sha512 https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys.sha512"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys"
        ExecStart=/bin/sh -c "cd /tmp/ && sha512sum -c authorized_keys.sha512 && cp authorized_keys /home/core/.ssh/authorized_keys && chmod 700 /home/core/.ssh && chmod 600 /home/core/.ssh/authorized_keys && chown -R core:core /home/core/.ssh"
        Restart=no