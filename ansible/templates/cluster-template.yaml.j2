# Unique name of Kubernetes cluster. In order to deploy
# more than one cluster into the same AWS account, this
# name must not conflict with an existing cluster.

clusterName: {{ stack_name }}

# The URI of the S3 bucket for the cluster
s3URI: s3://{{ provisioner_bucket }}

# CoreOS release channel to use. Currently supported options: alpha, beta, stable
# See coreos.com/releases for more information
releaseChannel: {{ coreos_release_channel}}

# The AMI ID of CoreOS.
# If omitted, the latest AMI for the releaseChannel is used.
# To update this to the latest AMI run the following command with the appropriate region and channel then place the resulting ID here
#   REGION=eu-west-1 && CHANNEL=stable && curl -s https://coreos.com/dist/aws/aws-$CHANNEL.json | jq -r ".\"$REGION\".hvm"
amiId: {{ amiId }}

# Container Linux has automatic updates https://coreos.com/os/docs/latest/update-strategies.html. This can be a risk in certain situations and this is why is disabled by default and you can enable it by setting this param to false.
disableContainerLinuxAutomaticUpdates: true

# TODO [SB] check this...
# Override the CloudFormation logical sub-stack names of control plane, etcd and/or network.
# Changing these names causes CF to delete the old and create a new stack and thus deleting and recreating all components in the stack.
#
# From kube-aws 0.11.x and onwards, separate networking and etcd CF stacks have been introduced.
# Because CF does not support migration of resources between stacks (or renaming stacks), a new VPC will be created if you upgrade an existing 0.10.x cluster.
# Creating a new VPC and deleting the old one can cause a lot of issues and might not be desired.
# To make this migration more feasible it is posible to change the name of the network stack to the old controleplane stack to keep the vpc components in the same stack. This way CF will not create a new vpc.
# In this scenario the name of the networkstack becomes "ControlPlane". (which is the name of the stack which holds the vpc components in a kube-aws pre 0.11 cluster)
# For clearity it is advised also rename the stack of the controlPlane components to, for example, "masters"
# Note that this will keep some legacy naming around in your configs and CF stacks...
# cloudformation:
#   stackNameOverrides:
#     controlPlane: "control-plane"
#     network: "network"
#     etcd: "etcd"

# The ID of hosted zone to add the externalDNSName to.
# Either specify hostedZoneId or hostedZone, but not both
#hostedZoneId: ""

# Network ranges of sources you'd like SSH accesses to be allowed from, in CIDR notation. Defaults to ["0.0.0.0/0"] which allows any sources.
# Explicitly set to an empty array to completely disable it.
# If you do that, probably you would like to set securityGroupIds to provide this worker node pool an existing SG with a SSH access allowed from specific ranges.
sshAccessAllowedSourceCIDRs:

# The name of one of API endpoints defined in `apiEndpoints` below to be written in kubeconfig and then used by admins
# to access k8s API from their laptops, CI servers, or etc.
# Required if there are 2 or more API endpoints defined in `apiEndpoints`
#adminAPIEndpointName: versionedPublic

# Kubernetes API endpoints with each one has a DNS name and is with/without a managed/unmanaged load balancer, Route 53 record set
# CAUTION: `externalDNSName` must be omitted when there are one or more items under `apiEndpoints`
apiEndpoints:
- # The unique name of this API endpoint used to identify it inside CloudFormation stacks or
  # to be referenced from other parts of cluster.yaml
  name: default

  # DNS name for this endpoint, added to the kube-apiserver TLS cert
  # It must be somehow routable to the Kubernetes controller nodes
  # from worker nodes and external clients. Configure the options
  # below if you'd like kube-aws to create a Route53 record sets/hosted zones
  # for you.  Otherwise the deployer is responsible for making this name routable

  dnsName: {{ stack_name }}-api.ft.com

  # Configuration for the load balancer serving this endpoint
  # Omit all the settings when you want kube-aws not to provision a load balancer for you
  loadBalancer:
    # Specifies an existing load-balancer used for load-balancing controller nodes and serving this endpoint
    # Setting id requires all the other settings excluding `name` to be omitted because reusing an ELB implies that configuring other resources
    # like a Route 53 record set for the endpoint is now your responsibility!
    # Also, don't forget to add controller.securityGroupIds to include a glue SG to allow your existing ELB to access controller nodes created by kube-aws
    #id: existing-elb

    # All the subnets assigned to this load-balancer. Specified only when this load balancer is not reused but managed one
    # Must be omitted when `id` is specified
    subnets:
    #- name: managedPublic1
    - name: ExistingPublicSubnet1
    - name: ExistingPublicSubnet2
    - name: ExistingPublicSubnet3

    # Set to true so that the managed ELB becomes an `internal` one rather than `internet-facing` one
    # When set to true while subnets are omitted, one or more private subnets in the top-level `subnets` must exist
    # Must be omitted when `id` is specified
    private: false

    # Set to 'network' to provision a Network Load Balancer instead of a classic ELB; this will cause
    # the controller nodes SG to allow inbound traffic from the VPC CIDR to port 443 TCP, see:
    # http://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html
    # Must be omitted when `id` is specified
    #type: classic

    # TTL in seconds for the Route53 RecordSet created if hostedZone.id is set to a non-nil value.
    #recordSetTTL: 300

    recordSetManaged: false

    # The Route 53 hosted zone is where the resulting Alias record is created for this endpoint
    #
    # Omitting hostedZone.id implies you must configure Route 53 hosted zone and record set or your own DNS so that worker nodes
    # are able to resolve DNS names of this API endpoint.
    #
    # Must be omitted when `id` is specified because kube-aws doesn't deal with the existing ELB by design
    #hostedZone:
    #  # The ID of hosted zone to add the dnsName to.
    #  id: ""
#    # Network ranges of sources you'd like Kubernetes API accesses to be allowed from, in CIDR notation. Defaults to ["0.0.0.0/0"] which allows any sources.
#    # Explicitly set to an empty array to completely disable it.
#    # If you do that, probably you would like to set securityGroupIds to provide this load balancer an existing SG with a Kubernetes API access allowed from specific ranges.
#    apiAccessAllowedSourceCIDRs:

#    # Existing security groups attached to this load balancer which are typically used to
#    # allow Kubernetes API accesses from admins and/or CD systems when `apiAccessAllowedSourceCIDRs` are explicitly set to an empty array
    # todo [customize] per account & region
    # The group id of aws-composer-custom-k8s-api-internal-access-sg*
    securityGroupIds:
    - {{ api_private_access_sg }}
#
#  #
#  # Common configuration #1: Unversioned, internet-facing API endpoint
#  #
#  - name: unversionedPublic
#    dnsName: api.example.com
#    loadBalancer:
#      id: elb-abcdefg
#
#  #
#  # Common configuration #2: Versioned, internet-facing API endpoint
#  #
#  - name: versionedPublic
#    dnsName: v1api.example.com
#    loadBalancer:
#      hostedZone:
#        id: hostedzone-abcedfg
#
#  #
#  # Common configuration #3: Unmanaged endpoint a.k.a Extra DNS name added to the kube-apiserver TLS cert
#  #
#  - name: extra
#    dnsName: youralias.example.com
#    loadBalancer:
#      managed: false
#
#  #
#  # Uncommon configuration #1: Managed, internet-facing API endpoint, without a Route53 record set
#  #
#  - name: elbOnly
#    dnsName: youralias.example.com
#    loadBalancer:
#      # Setting this to false allows you to omit hostedZone.id and hence the creation of Route53 record set is skipped
#      recordSetManaged: false
#

# Name of the SSH keypair already loaded into the AWS
# account being used to deploy this cluster.
# todo [customize] per stack
keyName: {{ debug_key_name }}

# Additional keys to preload on the coreos account (keep this to a minimum)
#sshAuthorizedKeys:
# - "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDPZPLtRvmeD8ZR/cm3SQ2GXc/tpDGPwP0slQDq3OV9R8+QiArvvfaGPpALM4xf3Y+DQxkG5W+PzPpYGUy4B17HVB9dywSZEjroqvf5/3LZp+X8dVQ/NPnOcox31zkmKhh8zODeWgcOv+tmf6hG82oGlMZci/6NMajpUT2RZYJfhN/aX+zeKkVZr1rckhuva7p0k3mcbtom9dMK0+W9rs2TjRTCrKo0l4jl9hhlc0omfRyQhvFmji/pgKoUoST9GFP7vS21QIAYR6K5PcIjYwn+p/jPKATHCD5PKHDilu+wt7GUaf3VyXljKavJsSAOydso+qsYivj/uwWhYfxppMTR sorin.buliarca@ft.com"

# Region to provision Kubernetes cluster
# todo [customize] per region
region: {{ region }}

# Availability Zone to provision Kubernetes cluster when placing nodes in a single availability zone (not highly-available) Comment out for multi availability zone setting and use the below `subnets` section instead.
#availabilityZone: eu-west-1a

# ARN of the KMS key used to encrypt TLS assets.
# todo [customize] per account & region
kmsKeyArn: "{{ kms_key_arn }}"

controller:
#  # Number of controller nodes to create, for more control use `controller.autoScalingGroup` and do not use this setting
#  count: {{ controller_count }}
#
#  # Maximum time to wait for controller creation
#  createTimeout: PT15M
#
#  # Instance type for controller node.
#  # CAUTION: Don't use t2.micro or the cluster won't work. See https://github.com/kubernetes/kubernetes/issues/18975
  instanceType: {{ controller_instance_type }}
#
#  # EC2 instance tags for controller nodes
#  instanceTags:
#    instanceRole: controller

#
#  rootVolume:
#    # Disk size (GiB) for controller node
#    size: 30
#    # Disk type for controller node (one of standard, io1, or gp2)
#    type: gp2
#    # Number of I/O operations per second (IOPS) that the controller node disk supports. Leave blank if controller.rootVolume.type is not io1
#    iops: 0
#
#  # Additional EBS volumes mounted on the controller(s)
#  # No additional EBS volumes by default. All parameter values do not default - they must be explicitly defined
#  volumeMounts:
#  - type: "gp2"
#    iops: 0
#    size: 30
#    # Follow the aws convention of '/dev/xvd*' where '*' is a single letter from 'f' to 'z'
#    device: "/dev/xvdf"
#    path: "/ebs"
#
#
#  # Existing security groups attached to controller nodes which are typically used to
#  # (1) allow access from controller nodes to services running on an existing infrastructure
#  # (2) allow ssh accesses from bastions when `sshAccessAllowedSourceCIDRs` are explicitly set to an empty array
  securityGroupIds:
    # allow SSH from internal VPC machines. Group name "Internal SSH security group"
    # todo [customize] per account & region
    - {{ vpc_internal_ssh_sg }}
#
#  # Auto Scaling Group definition for controllers. If only `controllerCount` is specified, min and max will be the set to that value and `rollingUpdateMinInstancesInService` will be one less.
  autoScalingGroup:
    minSize: {{ controller_count }}
    maxSize: {{ controller_count | int + 1 }}
    rollingUpdateMinInstancesInService: {{ controller_count }}

# TODO [SB] check this
  iam:
#    role:
#      # If you specify a name for the role, kube-aws will create it without a random id suffix (AWS default).
#      # Given a role of `yourManagedRole`, this will create an IAM role named `${clusterName}-${region}-yourManagedRole`
#      # to follow the recommendation in AWS documentation  http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html
#      # There are several cases you might need a stable name such as with .experimental.kube2IamSupport or .experimental.kiamSupport. See #297 for more information.
#      # ATTENTION: Consider limiting number of characters in clusterName and iam.role.name to avoid the resulting IAM
#      # role name's length from exceeding the AWS limit: 64. See https://github.com/kubernetes-incubator/kube-aws/issues/347
#      # It will have attached a customer Managed Policy that you can modify afterwards if you need more permissions for your cluster.
#      # Be careful with the Statements you modify because an update could overwrite your own.
#      # the Statements included in the ManagedPolicy are the minimun ones required for the Controllers to run.
#      name: "yourManagedRole"
#
#      # If strictName is enabled then the name specified above in "name" is not not altered by kube-aws.
#      # strictName is "false" by default which means that kube-aws will prepend the ian role name with the result of "ClusterName-AWSRegion-"
#      # strictName: true
#
#	     # If manageExternally is enabled (i.e set to true), kube-aws will not create a new IAM role and will instead use an existing one.
#	     # The existing role name must be set under iam.role.name and must exist on AWS before the cluster is rolled out.
#      # When using manageExternally, strictName is also assumed by kube-aws
#	     # By default, manageExternally is disabled (i.e set to false)
#	     manageExternally: true
#
#      # If you set managedPolicies here it will be attached in addition to the created managedPolicy in kube-aws for the cluster.
#      # CAUTION: if you attach a more restrictive policy in some resources (i.e ec2:* Deny) you can make kube-aws fail.
#      managedPolicies:
#      - arn: "arn:aws:iam::aws:policy/AdministratorAccess"
#      - arn: "arn:aws:iam::YOURACCOUNTID:policy/YOURPOLICYNAME"
#
#      # If you set an InstanceProfile kube-aws will NOT create any IAM Role and will use the configured `instanceProfile`.
#      # CAUTION: you must ensure that the IAM Role linked to the listed InstanceProfile has enough permissions to ensure kube-aws to run.
#      # if you dont know which permissions are required is recommended to create a cluster with a managed role.
    instanceProfile:
      # todo [customize] per account & region
      arn: "{{ controller_iam_role }}"

#  # If omitted, public subnets are created by kube-aws and used for controller nodes
  subnets:
#    # References subnets defined under the top-level `subnets` key by their names
    - name: ExistingPrivateSubnet1
    - name: ExistingPrivateSubnet2
    - name: ExistingPrivateSubnet3

#
#   # Kubernetes node labels to be added to controller nodes
  nodeLabels:
    kube-aws.coreos.com/role: controller
#
#  # User defined files that will be added to the Controller cluster cloud-init configuration in the "write_files:" section.
#  # Writing a kubernetes manifest to path /srv/kubernetes/manifests/custom/*.yaml will be automatically
#  # installed when the controllers start up.

  customFiles:
  - path: /etc/systemd/journald.conf.d/10-override-config.conf
    permissions: 0644
    content: |
      [Journal]
      MaxLevelConsole=crit
      Compress=false
      RateLimitInterval=0
      RateLimitBurst=0

# We started dropping packets because the netfilter conntrack table filled up.
# More info - https://trello.com/c/kuI6YJSv/747-packet-drop-investigation
# 
# The following changes:
# - decrease the TCP timeout for established connections
# - increase the max number of conntrack entries
# - increase the size of the hash table

# Ensure the nf_conntrack module is loaded before setting kernel parameters
  - path: /etc/modules-load.d/nf_conntrack.conf
    permissions: 0644
    content: |
      nf_conntrack

# Increase size of hash table
  - path: /etc/modprobe.d/nf_conntrack.conf
    permissions: 0644
    content: |
      options nf_conntrack hashsize=65536

# Lower TCP established timeout, increase max connections
  - path: /etc/sysctl.d/91-nf_conntrack.conf
    permissions: 0644
    content: |
      net.netfilter.nf_conntrack_tcp_timeout_established=300
      net.netfilter.nf_conntrack_max=262144

  - path: /etc/systemd/system/docker.service.d/99-docker-opts-override.conf
    permissions: 0644
    content: |
      [Service]
      Environment="{% raw %}DOCKER_OPTS=--log-driver=journald --host 0.0.0.0:2375 --log-opt tag={{.ImageName}}{% endraw %}"
#    - path: "/etc/rkt/auth.d/docker.json"
#      permissions: 0600
#      content: |
#        { "rktKind": "dockerAuth", "rktVersion": "v1", ... }
#
#  # User defined systemd units that will be added to the Controller cluster cloud-init configuration in the "units:" section.
  customSystemdUnits:

#   Reload systemd modules to pick up nf_conntrack hashsize parameter.
  - name: systemd-modules-load.service
    command: restart

#   Load the sysctl customizations. We need this so that the customizations in /etc/sysctl.d/ will be picked up.
  - name: sysctl-reload.service
    command: start
    runtime: true
    content: |
        [Unit]
        Description=Make sure we update sysctl settings

        [Service]
        Before=kubelet.service docker.service
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl --system"
        Restart=no

        [Install]
        WantedBy=multi-user.target
  - name: systemd-journald.service
    command: restart
  - name: authorized_keys.service
    command: start
    content: |
        [Unit]
        Description=Update authorized_keys
        [Service]
        Type=oneshot
        ExecStartPre=/bin/sh -c "mkdir -p /home/core/.ssh && touch /home/core/.ssh/authorized_keys"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys.sha512 https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys.sha512"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys"
        ExecStart=/bin/sh -c "cd /tmp/ && sha512sum -c authorized_keys.sha512 && cp authorized_keys /home/core/.ssh/authorized_keys && chmod 700 /home/core/.ssh && chmod 600 /home/core/.ssh/authorized_keys && chown -R core:core /home/core/.ssh"
        Restart=no
  - name: authorized_keys.timer
    command: start
    content: |
        [Unit]
        Description=Authorized keys timer
        [Timer]
        OnBootSec=1min
        OnUnitActiveSec=1min
        [Install]
        WantedBy=timers.target

#    - name: monitoring.service
#      command: start
#      enable: true
#      content: |
#        [Unit]
#        Description=Example Custom Service
#        [Service]
#        ExecStart=/bin/rkt run --set-env TAGS=Controller ...

worker:
#
#  # Settings that apply to all worker node pools
#
#  # The name of the API endpoint defined in the top level `apiEndpoints` that the worker node pools will use to access the k8s API
#  # Required if there are 2 or more API endpoints defined in `apiEndpoints`
#  # Can be overriden per node pool by specifying `worker.nodePools[].apiEndpointName`
#  apiEndpointName: versionedPublic
#
#  # Used to globally specify the strategy in which nodePools will roll out: in sequence or parallel
#  # Default behaviour ('parallel') will roll out nodePools concurrently
#  # If set to 'Sequential', the nodePools will roll out sequentially, in order of declaration
#  # If set at a Worker level, it will only take effect if it is not set at a nodePool level
#  # For finer grain control, declare it on a nodePool-by-nodePool basis
#  nodePoolRollingStrategy: Sequential

  nodePools:
{% for worker in worker_pools %}
    - # Name of this node pool. Must be unique among all the node pools in this cluster
      name: w{{ worker.id }}
#      # Subnet(s) to which worker nodes in this node pool are deployed
#      # References subnets defined under the top-level `subnets` key by their names
#      # If omitted, public subnets are created by kube-aws and used for worker nodes
      subnets:
#      - # References subnets defined under the top-level `subnets` key by their names
{% set subnet_count = worker.subnets | int %}
{% for n in range(1, subnet_count+1) %}
       - name: ExistingPrivateSubnet{{ n }}
{% endfor %}
#       - name: ManagedPublicSubnet1
#      # Used to specify the strategy in which the nodePool will roll out: in sequence or parallel
#      # By default (if not set at nodePool/worker level) the nodePool will roll out in parallel
#      # If set to 'Sequential', the nodePools will roll out one-by-one in order of declaration
#      # NOTICE: if there are multiple nodePool groups that will use both rolling strategies ('Sequential'/'Parallel')
#      # the nodePools rolling out in sequence should be declared before those that roll out in parallel
#      # otherwise, the first declared nodePool to roll out in sequence will depend on the last concurrent nodePool
#      nodePoolRollingStrategy: Parallel

#
#      # Existing "glue" security groups attached to worker nodes which are typically used to allow
#      # access from worker nodes to services running on an existing infrastructure
      securityGroupIds:
        # Allow ssh from internal VPC. Group name: "Internal SSH security group"
        # todo [customize] per account & region
{% for sg in worker_security_groups %}
        - {{ sg }}
{% endfor %}
#
#      # Configuration for external managed ELBs for worker nodes
#      # Use this with k8s load balancers with type=NodePort. See https://kubernetes.io/docs/user-guide/services/#type-nodeport
#      #
#      # NOTICE: This is generally recommended over k8s managed load-balancers with type=LoadBalancer because it allows
#      # a manual but no-downtime rotation of kube-aws clusters
#      loadBalancer:
#        enabled: true
#        # Names of ELBs attached to worker nodes
#        names: [ "manuallymanagedelb" ]
#        # IDs of "glue" security groups attached to worker nodes to allow the ELBs to communicate with worker nodes
#        securityGroupIds: [ "sg-87654321" ]
#
#      # The name of the API endpoint defined in the top level `apiEndpoints` that this worker node pool will use to access the k8s API
#      # If not specified, defaults to `worker.apiEndpointName`
#      apiEndpointName: versionedPublic
#
      iam:
#        role:
#          # If you specify a name for the role, kube-aws will create it without a random id suffix (AWS default).
#          # Given a role of `yourManagedRole`, this will create an IAM role named `${clusterName}-${region}-yourManagedRole`
#          # to follow the recommendation in AWS documentation  http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html
#          # There are several cases you might need a stable name such as with .experimental.kube2IamSupport or .experimental.kiamSupport. See #297 for more information.
#          # ATTENTION: Consider limiting number of characters in clusterName and iam.role.name to avoid the resulting IAM
#          # role name's length from exceeding the AWS limit: 64. See https://github.com/kubernetes-incubator/kube-aws/issues/347
#          # It will have attached a customer Managed Policy that you can modify afterwards if you need more permissions for your cluster.
#          # Be careful with the Statements you modify because an update could overwrite your own.
#          # the Statements included in the ManagedPolicy are the minimun ones required for the Workers to run.
#          name: "yourManagedRole"
#
#          # If you set managedPolicies here it will be attached in addition to the created managedPolicy in kube-aws for the cluster.
#          # CAUTION: if you attach a more restrictive policy in some resources (i.e ec2:* Deny) you can make kube-aws fail.
#          managedPolicies:
#          - arn: "arn:aws:iam::aws:policy/AdministratorAccess"
#          - arn: "arn:aws:iam::YOURACCOUNTID:policy/YOURPOLICYNAME"
#
#          # If you set an InstanceProfile kube-aws will NOT create any IAM Role and will use the configured `instanceProfile`.
#          # CAUTION: you must ensure that the IAM Role linked to the listed InstanceProfile has enough permissions to ensure kube-aws to run.
#          # if you dont know which permissions are required is recommended to create a cluster with a managed role.

        # todo [customize] per account & region
        instanceProfile:
          arn: "{{ worker_iam_role }}"
#      # Configuration for external managed ALBs Target Groups for worker nodes
#      targetGroup:
#        enabled: true
#        # ARNs of ALBs Target Groups attached to worker nodes
#        arns:
#        - arn:aws:elasticloadbalancing:eu-west-1:xxxxxxxxxxxx:targetgroup/manuallymanagedetg/xxxxxxxxxxxxxxxx
#        # IDs of "glue" security groups attached to worker nodes to allow the ALBs Target Groups to communicate with worker nodes
#        securityGroupIds: [ "sg-87654321" ]
#
#      # Additional EBS volumes mounted on the worker
#      # No additional EBS volumes by default. All parameter values do not default - they must be explicitly defined
#      volumeMounts:
#      - type: "gp2"
#        iops: 0
#        size: 30
#        # Follow the aws convention of '/dev/xvd*' where '*' is a single letter from 'f' to 'z'
#        device: "/dev/xvdf"
#        path: "/ebs"
#
#      # Additional Raid0 EBS backed volumes mounted on the worker
#      # No additional Raid0 volumes by default. All parameter values do not default - they must be explicitly defined
#      raid0Mounts:
#      - type: "gp2"
#        iops: 0
#        # size of each device. As below shows 4 devices, total size is 4 * 214G or 896G.
#        size: 214
#        # Follow the aws convention of '/dev/xvd*' where '*' is a single letter from 'f' to 'z'
#        devices:
#        - "/dev/xvdg"
#        - "/dev/xvdh"
#        - "/dev/xvdi"
#        - "/dev/xvdj"
#        path: "/data"
#
#      # Specifies how often kubelet posts node status to master. Note: be cautious when changing the constant, it must work with nodeMonitorGracePeriod in nodecontroller.
#      # nodeStatusUpdateFrequency: "10s"
#
#      #
#      # Settings only for ASG-based node pools
#      #
#
#      # Number of worker nodes to create for an autoscaling group based pool
#      # Specify `worker.autoScalingGroup` instead for more control over its min/max/desired capacity.
#      count: {{ worker.count }}
#      # Instance type for worker nodes
#      # CAUTION: Don't use t2.micro or the cluster won't work. See https://github.com/kubernetes/kubernetes/issues/16122
      instanceType: {{ worker.instance_type }}
#      # EC2 instance tags for worker nodes
#      instanceTags:
#        instanceRole: worker
#
      rootVolume:
#        # Disk size (GiB) for worker nodes
        size: {{ worker.rootVolumeSize | default ('30') }}
#        # Disk type for worker node (one of standard, io1, or gp2)
#        type: gp2
#        # Number of I/O operations per second (IOPS) that the worker node disk supports. Leave blank if worker.rootVolume.type is not io1
#        iops: 0
#
#      # Maximum time to wait for worker creation
#      createTimeout: PT15M
#
#      # Tenancy of the worker nodes. Options are "default" and "dedicated"
#      # Documentation: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/dedicated-instance.html
#      tenancy: default
#
#      # (Experimental) GPU Driver installation support
#      # Currently, only Nvidia driver is supported.
#      # This setting takes effect only when configured instance type is GPU enabled (p2, p3, g2, or g3).
#      # Make sure to choose 'docker' as container runtime when enabled this feature.
#      # Ensure that automatic Container Linux is disabled(it is disabled by default btw).
#      # Otherwise the installed driver may stop working when an OS update resulted in an updated kernel
#      gpu:
#        nvidia:
#          enabled: true
#          version: "375.66"
#
#      # Price (Dollars) to bid for spot instances. Omit for on-demand instances.
#      spotPrice: "0.05"
#
#      # When enabled, autoscaling groups managing worker nodes wait for nodes to be up and running.
#      # This is enabled by default.
#      waitSignal:
#        enabled: true
#        # Max number of nodes concurrently updated
#        maxBatchSize: 1
#
#      # Auto Scaling Group definition for workers. If only `workerCount` is specified, min and max will be the set to that value and `rollingUpdateMinInstancesInService` will be one less.
      autoScalingGroup:
        minSize: {{ worker.count }}
        maxSize: {{ worker.count | int + 1 }}
        rollingUpdateMinInstancesInService: {{ worker.count }}
#
#      #
#      # Spot fleet config for worker nodes
#      #
#      spotFleet:
#        # Total desired number of units to maintain
#        # An unit is chosen by you and can be a vCPU, specific amount of memory, size of instance store, etc., according to your requirement.
#        # Omit or put zero to disable the spot fleet support and use an autoscaling group to manage nodes.
#        targetCapacity: 10
#
#        # IAM role to grant the Spot fleet permission to bid on, launch, and terminate instances on your behalf
#        # See http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-requests.html#spot-fleet-prerequisites
#        #
#        # Defaults to "arn:aws:iam::youraccountid:role/aws-ec2-spot-fleet-tagging-role" assuming you've arrived "Spot Requests" in EC2 Dashboard
#        # hence the role is automatically created for you
#        iamFleetRoleArn: "arn:aws:iam::youraccountid:role/kube-aws-doesnt-create-this-for-you"
#
#        # Price per unit hour = Price per instance hour / num of weighted capacity for the instance
#        # Defaults to "0.06"
#        spotPrice: 0.06
#
#        # Disk size (GiB) per unit
#        # Disk size for each launch specification defaults to unitRootVolumeSize * weightedCapacity
#        unitRootVolumeSize: 30
#
#        # Disk type for worker node (one of standard, io1, or gp2)
#        # Can be overridden in each launch specification
#        rootVolumeType: gp2
#
#        # Number of I/O operations per second (IOPS) per unit. Leave blank if rootVolumeType is not io1
#        # IOPS for each launch specification defaults to unitRootVolumeIOPS * weightedCapacity
#        unitRootVolumeIOPS: 0
#
#        launchSpecifications:
#        - # Number of units provided by an EC2 instance of the instanceType.
#          weightedCapacity: 1
#          instanceType: c4.large
#
#          # Price per instance hour
#          # Defaults to worker.spotFleet.spotPrice * weightedCapacity if omitted
#          #spotPrice:
#
#          rootVolume:
#            # Defaults to worker.spotFleet.rootVolumeType if omitted
#            type:
#            # Defaults to worker.spotFleet.unitRootVolumeSize * weightedCapacity if omitted
#            size:
#            # Number of provisioned IOPS when rootVlumeType is io1
#            # Must be within the range between 100 and 20000
#            # Defaults to worker.spotFleet.unitRootVolumeIOPS * weightedCapacity if omitted
#            iops:
#
#        - weightedCapacity: 2
#          instanceType: c4.xlarge
#
#      #
#      # Optional settings for both ASG-based and SpotFleet-based node pools
#      #
#
#      # Autoscaling by adding/removing nodes according to resource usage
      autoscaling:
#        # Make this node pool an autoscaling-target of k8s cluster-autoscaler
#        #
#        # Beware that making this an autoscaing-target doesn't automatically deploy cluster-autoscaler itself -
#        # turn on `addons.clusterAutoscaler.enabled` to deploy it on controller nodes.
        clusterAutoscaler:
          enabled: false
#
#      # Used to provide `/etc/environment` env vars with values from arbitrary CloudFormation refs
#      awsEnvironment:
#        enabled: true
#        environment:
#          CFNSTACK: '{ "Ref" : "AWS::StackId" }'
#
#      # Add predefined set of labels to the nodes
#      # The set includes names of launch configurations and autoscaling groups
      awsNodeLabels:
        enabled: true
#      # Provision worker nodes with IAM permissions and node labels to run cluster-autoscaler (assuming `addons.clusterAutoscaler.enabled` is true)
      clusterAutoscalerSupport:
        enabled: false
#
#      # Mount an EFS only to a specific node pool.
#      # This is a NFS share that will be available across a node pool through a hostPath volume on the "/efs" mountpoint
#      #
#      # If you'd like to configure an EFS mounted to every node, use the top-level `elasticFileSystemId` instead
#      #
#      # Beware that currently it doesn't work for a node pool in subnets managed by kube-aws.
#      # In other words, this node pool must be in existing subnets managed by you by specifying subnets[].id to make this work.
#      elasticFileSystemId: fs-47a2c22e
#
#      # This option has not yet been tested with rkt as container runtime
#      ephemeralImageStorage:
#        enabled: true
#
#      # When enabled this will grant sts:assumeRole permission to the IAM roles for worker nodes in this pool.
#      # This is intended to be used in combination with nodePools[].iam.role.name. See #297 for more information.
#      kube2IamSupport:
#        enabled: true
#
#      # Propagate custom CLI options to kubelet
#      # Provided example overrides default docker image garbage collection limits
#      # as in https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/
#      # Anything from https://kubernetes.io/docs/admin/kubelet/ can be used as a value

      kubeletOpts: --image-gc-low-threshold=65 --image-gc-high-threshold=75 --enable-load-reader=true

#      # Kubernetes node labels to be added to worker nodes
      nodeLabels:
        kube-aws.coreos.com/role: {{ worker.role }}
{% if worker.dedicatedtaint is defined %}
      taints:
        - key: dedicated
          value: {{ worker.dedicatedtaint }}
          effect: NoSchedule
{% endif %}
#      # Other less common customizations per node pool
#      # All these settings default to the top-level ones
#      keyName:
#      releaseChannel: alpha
#      amiId:
#      kubernetesVersion: 1.6.0-alpha.1
#
#      # Images are taken from controlplane by default, but you can override values for node pools here. E.g.:
#      AwsCliImage:
#        repo: quay.io/coreos/awscli
#        tag: edge
#        rktPullDocker: false
#
#      sshAuthorizedKeys:
#      # User-provided YAML map available in control-plane's stack-template.json
#      customSettings:
#        key1: [ 1, 2, 3 ]


#      # User defined files that will be added to the NodePool cloud-init configuration in the "write_files:" section.
      customFiles:
      - path: /etc/systemd/journald.conf.d/10-override-config.conf
        permissions: 0644
        content: |
          [Journal]
          MaxLevelConsole=crit
          Compress=false
          RateLimitInterval=0
          RateLimitBurst=0

#  Tuned the net.ipv4.tcp_retries2 kernel param on the worker nodes, so that stale TCP connections will be closed in ~ 1min.
#  Since the kubelet holds a long TCP connection to the API server for posting its status, if that connection gets stalled, we'll get a NodeNotReady
#  The default was 15 which caused the stale connections to be closed in ~ 15 mins.
#  More on this here:  https://github.com/kubernetes/kubernetes/pull/48670, https://github.com/kubernetes/kubernetes/issues/41916#issuecomment-312428731
      - path: /etc/sysctl.d/90-tcp-retries.conf
        permissions: 0644
        content: |
          net.ipv4.tcp_retries2=6
          net.ipv4.tcp_keepalive_time=30
          net.ipv4.tcp_keepalive_probes=4
          net.ipv4.tcp_keepalive_intvl=5
      - path: /etc/sysctl.d/91-inotify-max.conf
        permissions: 0644
        content: |
          fs.inotify.max_user_instances=256

# We started dropping packets because the netfilter conntrack table filled up.
# More info - https://trello.com/c/kuI6YJSv/747-packet-drop-investigation
# 
# The following changes:
# - decrease the TCP timeout for established connections
# - increase the max number of conntrack entries
# - increase the size of the hash table

# Ensure the nf_conntrack module is loaded before setting kernel parameters
      - path: /etc/modules-load.d/nf_conntrack.conf
        permissions: 0644
        content: |
          nf_conntrack

# Increase size of hash table
      - path: /etc/modprobe.d/nf_conntrack.conf
        permissions: 0644
        content: |
          options nf_conntrack hashsize=65536

# Lower TCP established timeout, increase max connections
      - path: /etc/sysctl.d/91-nf_conntrack.conf
        permissions: 0644
        content: |
          net.netfilter.nf_conntrack_tcp_timeout_established=300
          net.netfilter.nf_conntrack_max=262144

      - path: /etc/systemd/system/docker.service.d/99-docker-opts-override.conf
        permissions: 0644
        content: |
          [Service]
          Environment="{% raw %}DOCKER_OPTS=--log-driver=journald --host 0.0.0.0:2375 --log-opt tag={{.ImageName}}{% endraw %}"

#        - path: "/etc/rkt/auth.d/docker.json"
#          permissions: 0600
#          content: |
#            { "rktKind": "dockerAuth", "rktVersion": "v1", ... }
#
#      # User defined systemd units that will be added to the NodePool cluster cloud-init configuration in the "units:" section.
      customSystemdUnits:
      - name: systemd-journald.service
        command: restart

#     Reload systemd modules to pick up nf_conntrack hashsize parameter.
      - name: systemd-modules-load.service
        command: restart

#     Load the sysctl customizations. We need this so that the customizations in /etc/sysctl.d/ will be picked up.
      - name: sysctl-reload.service
        command: start
        runtime: true
        content: |
            [Unit]
            Description=Make sure we update sysctl settings

            [Service]
            Before=kubelet.service docker.service
            Type=oneshot
            ExecStart=/bin/sh -c "sysctl --system"
            Restart=no

            [Install]
            WantedBy=multi-user.target
      - name: authorized_keys.service
        command: start
        content: |
            [Unit]
            Description=Update authorized_keys

            [Service]
            Type=oneshot
            ExecStartPre=/bin/sh -c "mkdir -p /home/core/.ssh && touch /home/core/.ssh/authorized_keys"
            ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys.sha512 https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys.sha512"
            ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys"
            ExecStart=/bin/sh -c "cd /tmp/ && sha512sum -c authorized_keys.sha512 && cp authorized_keys /home/core/.ssh/authorized_keys && chmod 700 /home/core/.ssh && chmod 600 /home/core/.ssh/authorized_keys && chown -R core:core /home/core/.ssh"
            Restart=no
      - name: authorized_keys.timer
        command: start
        content: |
            [Unit]
            Description=Authorized keys timer

            [Timer]
            OnBootSec=1min
            OnUnitActiveSec=1min

            [Install]
            WantedBy=timers.target
{% endfor %}

#        - name: monitoring.service
#          command: start
#          enable: true
#          content: |
#            [Unit]
#            Description=Example Custom Service
#            [Service]
#            ExecStart=/bin/rkt run --set-env TAGS=Controller ...
#      # Enable feature-gates such as:
#      # PodPriority (it only makes sense if you enabled priority admission control in experimental section)
#      # ExpandPersistentVolumes (it only makes sense if you enabled PersistentVolumeClaimResize admission control in experimental section)
#      featureGates:
#        PodPriority: true
#        ExpandPersistentVolumes: true

# Maximum time to wait for worker creation
#workerCreateTimeout: PT15M

# Instance type for worker nodes
# CAUTION: Don't use t2.micro or the cluster won't work. See https://github.com/kubernetes/kubernetes/issues/16122
#workerInstanceType: t2.medium

# Disk size (GiB) for worker nodes
#workerRootVolumeSize: 30

# Disk type for worker node (one of standard, io1, or gp2)
#workerRootVolumeType: gp2

# Number of I/O operations per second (IOPS) that the worker node disk supports. Leave blank if workerRootVolumeType is not io1
#workerRootVolumeIOPS: 0

# Tenancy of the worker nodes. Options are "default" and "dedicated"
# Documentation: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/dedicated-instance.html
# workerTenancy: default

# Price (Dollars) to bid for spot instances. Omit for on-demand instances.
# workerSpotPrice: "0.05"

## Etcd Cluster config
## WARNING: Any changes to etcd parameters after the cluster is first created will not be applied
## during a cluster upgrade, due to concerns over data loss.
## This situation is being rectified with work towards automated management of etcd clusters

etcd:
#  # Number of etcd nodes
#  # (Set to an odd number >= 3 for HA control plane)
#  # WARNING: You can't add etcd nodes after the first creation by modifying this and running `kube-aws update`
#  # See https://github.com/kubernetes-incubator/kube-aws/issues/631 for more information
  count: {{ etcd_count }}
#
#  # Instance type for etcd node
  instanceType: {{ etcd_instance_type }}
#  # EC2 instance tags for etcd nodes
#  instanceTags:
#    instanceRole: etcd

#
#  rootVolume:
#    # Root volume size (GiB) for etcd node
#    size: 30
#    # Root volume type for etcd node (one of standard, io1, or gp2)
#    type: gp2
#    # Number of I/O operations per second (IOPS) that the etcd node's root volume supports. Leave blank if etcdRootVolumeType is not io1
#    iops: 0
#
#  dataVolume:
#    # Data volume size (GiB) for etcd node
#    # if etcdDataVolumeEphemeral=true, this value is ignored. The size of ephemeral volumes is not configurable.
#    size: 30
#    # Data volume type for etcd node (one of standard, io1, or gp2)
#    type: gp2
#    # Number of I/O operations per second (IOPS) that the etcd node's data volume supports. Leave blank if etcdDataVolumeType is not io1
#    iops: 0
#    # Encrypt the Etcd Data volume.  Set to true for encryption.  This does not work for etcdDataVolumeEphemeral.
#    encrypted: false
#    # Use ephemeral instance storage for etcd data volume instead of EBS?
#    # (Recommended set to true for high-throughput control planes)
#    # CAUTION: Currently broken. Please don't turn this on until it is fixed in https://github.com/kubernetes-incubator/kube-aws/pull/417
#    ephemeral: false
#
#  # Additional EBS volumes mounted on the etcd
#  # No additional EBS volumes by default. All parameter values do not default - they must be explicitly defined
#  volumeMounts:
#  - type: "gp2"
#    iops: 0
#    size: 30
#    # Follow the aws convention of '/dev/xvd*' where '*' is a single letter from 'f' to 'z'
#    device: "/dev/xvdf"
#    path: "/ebs"
#
#  # Tenancy of the etcd instances. Options are "default" and "dedicated"
#  # Documentation: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/dedicated-instance.html
#  tenancy: default
#
#  # If omitted, public subnets are created by kube-aws and used for etcd nodes
  subnets:
    # References subnets defined under the top-level `subnets` key by their names
    - name: ExistingPrivateSubnet1
    - name: ExistingPrivateSubnet2
    - name: ExistingPrivateSubnet3

#
#  # Existing security groups attached to etcd nodes which are typically used to
#  # allow ssh accesses from bastions when `sshAccessAllowedSourceCIDRs` are explicitly set to an empty array
  securityGroupIds:
    # Allow SSH from internal VPC ips. Group name: "Internal SSH security group"
    # todo [customize] per account & region
    - {{ vpc_internal_ssh_sg }}
#    - sg-5678efab
#
#  # If you omit this block kube-aws would create an IAM Role and a managed policy for etcd nodes with a random name.
  iam:
#    role:
#      # If you set managedPolicies here they will be attached to the role in addition to the managed policy.
#      # CAUTION: if you attach a more restrictive policy in some resources (i.e ec2:* Deny) you can make kube-aws fail.
#      managedPolicies:
#      - arn: "arn:aws:iam::aws:policy/AdministratorAccess"
#      - arn: "arn:aws:iam::YOURACCOUNTID:policy/YOURPOLICYNAME"
#    # If you set an InstanceProfile kube-aws will NOT create any IAM Role and will use the existing instance profile.
#    # CAUTION: you must ensure that the IAM role linked to the existing instance profile has enough permissions to ensure etcd nodes to run.
#    # If you dont know which permissions are required, it is recommended to go ahead with a managed instance profile.
    # todo [customize] per account & region
    instanceProfile:
      arn: "{{ etcd_iam_role }}"
#
#  # The version of etcd to be used. Set to e.g. "3.2.1" to use etcd3
  version: 3.2.13

  snapshot:
#    # Set to true to periodically take an etcd snapshot
#    # Beware that this can be enabled only for etcd 3+
#    # Please carefully test if it works as you've expected when being enabled for your production clusters
    automated: true
#
#  disasterRecovery:
#    # Set to true to automatically execute a disaster-recovery process whenever etcd node(s) seemed to be broken for a while
#    # Beware that this can be enabled only for etcd 3+
#    # Please carefully test if it works as you've expected when being enabled for your production clusters
#    automated: false
#
#  # The strategy to provide your etcd nodes, in combination with floating EBS volumes, stable member identities. Defaults to "eip".
#  #
#  # Available options: eip, eni
#  #
#  # With every option, etcd nodes communicate to each other via their private IPs.
#  #
#  # eip: Use EC2 public hostnames stabilized with EIPs and "resolved eventually to private IPs"
#  #      Requires Amazon DNS (at the second IP of your VPC, e.g. 10.0.0.2) to work.
#  #      1st recommendation because less moving parts and relatively easy disaster recovery for a single-AZ etcd cluster.
#  #      If you run a single-AZ etcd cluster and the AZ failed, EBS volumes created from latest snapshots and EIPs can be reused in an another AZ to reproduce your etcd cluster in the AZ.
#  #
#  # eni: [EXPERIMENTAL] Use secondary ENIs and Route53 record sets to provide etcd nodes stable hostnames
#  #      Requires Amazon DNS (at the second IP of your VPC, e.g. 10.0.0.2) or an another DNS which can resolve dns names in the hosted zone managed by kube-aws to work.
#  #      2nd recommendation because relatively easy disaster recovery for a single-AZ etcd cluster but more moving parts than "eip".
#  #      If you run a single-AZ etcd cluster and the AZ failed, EBS volumes created from latest snapshots and record sets can be reused in an another AZ to reproduce your etcd cluster in the AZ.
#  #
  memberIdentityProvider: eip
#
#  # Domain of the hostname used for etcd peer discovery.
#  # Used only when `memberIdentityProvider: eni` for TLS key/cert generation
#  # If omitted, defaults to "ec2.internal" for us-east-1 region and "<region>.compute.internal" otherwise
#  internalDomainName:
#
#  # Set to `false` to disable creation of record sets.
#  # Used only when `memberIdentityProvider` is set to `eni`
#  # When disabled, it's your responsibility to configure all the etcd nodes so that
#  # they can resolve each other's FQDN(specified via the below `etcd.nods[].fqdn` settings) via your DNS(can be the Amazon DNS or your own DNS. Configure it with e.g. coroes-cloudinit)
#  manageRecordSets:
#
#  # Advanced configuration used only when `memberIdentityProvider: eni`
#  hostedZone:
#    # The hosted zone where record sets for etcd nodes managed by kube-aws are created
#    # If omitted, kube-aws creates a hosted zone for you
#    id:
#
#  # CAUTION: Advanced configuration. This should be omitted unless you have very deep knowledge of etcd and kube-aws
#  nodes:
#  - # The name of this etcd node. Specified only when you want to customize the etcd member's name shown in ETCD_INITIAL_CLUSTER and ETCD_NAME
#    name: etcd0
#    # The FQDN of this etcd node
#    # Usually this should be omitted so that kube-aws can choose a proper value.
#    # Specified only when `memberIdentityProvider: eni` and `manageRecordSets: false` i.e.
#    # it is your responsibility to properly configure EC2 instances to use a DNS which is able to resolve the FQDN.
#    fqdn: etcd0.<internalDomainName>
#  - name: etcd1
#    fqdn: etcd1.<internalDomainName>
#  - name: etcd2
#    fqdn: etcd2.<internalDomainName>
#
#  # ARN of the KMS key used to encrypt the etcd data volume. The account default key will be used if `etcdDataVolumeEncrypted`
#  # is enable and `etcd.kmsKeyArn` is omitted.
#  kmsKeyArn: ""
#
#  # User defined files that will be added to the Etcd cluster cloud-init configuration in the "write_files:" section.
#  customFiles:
#    - path: "/etc/rkt/auth.d/docker.json"
#      permissions: 0600
#      content: |
#        { "rktKind": "dockerAuth", "rktVersion": "v1", ... }
#
  customFiles:
# We started dropping packets because the netfilter conntrack table filled up.
# More info - https://trello.com/c/kuI6YJSv/747-packet-drop-investigation
# 
# The following changes:
# - decrease the TCP timeout for established connections
# - increase the max number of conntrack entries
# - increase the size of the hash table

# Ensure the nf_conntrack module is loaded before setting kernel parameters
  - path: /etc/modules-load.d/nf_conntrack.conf
    permissions: 0644
    content: |
      nf_conntrack

# Increase size of hash table
      - path: /etc/modprobe.d/nf_conntrack.conf
        permissions: 0644
        content: |
          options nf_conntrack hashsize=65536

# Lower TCP established timeout, increase max connections
  - path: /etc/sysctl.d/91-nf_conntrack.conf
    permissions: 0644
    content: |
      net.netfilter.nf_conntrack_tcp_timeout_established=300
      net.netfilter.nf_conntrack_max=262144

#  # User defined systemd units that will be added to the Etcd cluster cloud-init configuration in the "units:" section.
  customSystemdUnits:

#   Reload systemd modules to pick up nf_conntrack hashsize parameter.
  - name: systemd-modules-load.service
    command: restart

#   Load the sysctl customizations. We need this so that the customizations in /etc/sysctl.d/ will be picked up.
  - name: sysctl-reload.service
    command: start
    runtime: true
    content: |
        [Unit]
        Description=Make sure we update sysctl settings

        [Service]
        Before=kubelet.service docker.service
        Type=oneshot
        ExecStart=/bin/sh -c "sysctl --system"
        Restart=no

        [Install]
        WantedBy=multi-user.target
  - name: authorized_keys.service
    command: start
    content: |
        [Unit]
        Description=Update authorized_keys
        [Service]
        Type=oneshot
        ExecStartPre=/bin/sh -c "mkdir -p /home/core/.ssh && touch /home/core/.ssh/authorized_keys"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys.sha512 https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys.sha512"
        ExecStart=/bin/sh -c "curl -sSL --retry 5 --retry-delay 2 -o /tmp/authorized_keys https://raw.githubusercontent.com/Financial-Times/up-ssh-keys/master/authorized_keys"
        ExecStart=/bin/sh -c "cd /tmp/ && sha512sum -c authorized_keys.sha512 && cp authorized_keys /home/core/.ssh/authorized_keys && chmod 700 /home/core/.ssh && chmod 600 /home/core/.ssh/authorized_keys && chown -R core:core /home/core/.ssh"
        Restart=no
  - name: authorized_keys.timer
    command: start
    content: |
        [Unit]
        Description=Authorized keys timer
        [Timer]
        OnBootSec=1min
        OnUnitActiveSec=1min
        [Install]
        WantedBy=timers.target

#    - name: monitoring.service
#      command: start
#      enable: true
#      content: |
#        [Unit]
#        Description=Example Custom Service
#        [Service]
#        ExecStart=/bin/rkt run --set-env TAGS=Controller ...
#  # extra etcd options
#  userSuppliedArgs:
#    # for example, setting the --quota-backend-bytes and --auto-compaction-retention options
#    quotaBackendBytes:
#    autoCompactionRetention:


## Networking config

# CAUTION: Deprecated and will be removed in v0.9.9. Please use vpc.id instead
# vpcId: vpc-f75fb790

vpc:
#  # ID of existing VPC to create subnet in. Leave blank to create a new VPC
  # todo [customize] per account & region
  id: {{ vpc_id }}
#  # Exported output's name from another stack
#  # Only specify either id or idFromStackOutput but not both
#  idFromStackOutput: VpcId

# CAUTION: Deprecated and will be removed in v0.9.9. Please use internetGateway.id instead
# internetGatewayId:

#internetGateway:
#  # ID of existing Internet Gateway to associate subnet with. Leave blank to create a new Internet Gateway
#  id:
#  # Exported output's name from another stack
#  # Only specify either id or idFromStackOutput but not both
#  #idFromStackOutput: myinfra-igw

# Advanced: ID of existing route table in existing VPC to attach subnet to.
# Leave blank to use the VPC's main route table.
# This should be specified if and only if vpcId is specified.
#
# IMPORTANT NOTICE:
#
# If routeTableId is specified, it's your responsibility to add an appropriate route to
# an internet gateway(IGW) or a NAT gateway(NGW) to the route table.
#
# More concretely,
# * If you like to make all the subnets private, pre-configure an NGW yourself and add a route to the NGW beforehand
# * If you like to make all the subnets public, pre-configure an IGW yourself and add a route to the IGW beforehand
# * If you like to mix private and public subnets, omit routeTableId but specify subnets[].routeTable.id per subnet
#
# routeTableId: rtb-xxxxxxxx

# CIDR for Kubernetes VPC. If vpcId is specified, must match the CIDR of existing vpc.
# todo [customize] per account & region
vpcCIDR: "{{ vpc_cidr }}"

# CIDR for Kubernetes subnet when placing nodes in a single availability zone (not highly-available) Leave commented out for multi availability zone setting and use the below `subnets` section instead.
# instanceCIDR: "10.0.0.0/24"

# Kubernetes subnets with their CIDRs and availability zones.
# Differentiating availability zone for 2 or more subnets result in high-availability (failures of a single availability zone won't result in immediate downtimes)
subnets:
#   #
#   # Managed public subnet managed by kube-aws
#   #
#   - name: ManagedPublicSubnet1
#     # Set to false if this subnet is public
#     # private: false
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.0.0/24"
#
#   #
#   # Managed private subnet managed by kube-aws
#   #
#   - name: ManagedPrivateSubnet1
#     # Set to true if this subnet is private
#     private: true
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#
#   #
#   # Advanced: Unmanaged/existing public subnet reused but not managed by kube-aws
#   #
#   # An internet gateway(igw) and a route table contains the route to the igw must have been properly configured by YOU.
#   # kube-aws tries to reuse the subnet specified by id or idFromStackOutput but kube-aws never modify the subnet
#   #

   - name: ExistingPublicSubnet1
#     # Beware that `availabilityZone` can't be omitted; it must be the one in which the subnet exists.
     availabilityZone: {{ region }}a
#     # ID of existing subnet to be reused.
#     # availabilityZone should still be provided but instanceCIDR can be omitted when id is specified.
     # todo [customize] per account & region
     id: "{{ public_subnet_zone_a }}"
#     # Exported output's name from another stack
#     # Only specify either id or idFromStackOutput but not both
#     #idFromStackOutput: myinfra-PublicSubnet1

   - name: ExistingPublicSubnet2
#     # Beware that `availabilityZone` can't be omitted; it must be the one in which the subnet exists.
     availabilityZone: {{ region }}b
#     # ID of existing subnet to be reused.
#     # availabilityZone should still be provided but instanceCIDR can be omitted when id is specified.
    # todo [customize] per account & region
     id: "{{ public_subnet_zone_b }}"
#     # Exported output's name from another stack
#     # Only specify either id or idFromStackOutput but not both
#     #idFromStackOutput: myinfra-PublicSubnet1
   - name: ExistingPublicSubnet3
#     # Beware that `availabilityZone` can't be omitted; it must be the one in which the subnet exists.
     availabilityZone: {{ region }}c
#     # ID of existing subnet to be reused.
#     # availabilityZone should still be provided but instanceCIDR can be omitted when id is specified.
     # todo [customize] per account & region
     id: "{{ public_subnet_zone_c }}"

   - name: ExistingPrivateSubnet1
     availabilityZone: {{ region }}a
     # todo [customize] per account & region
     id: "{{ private_subnet_zone_a }}"
   - name: ExistingPrivateSubnet2
     availabilityZone: {{ region }}b
     # todo [customize] per account & region
     id: "{{ private_subnet_zone_b }}"
   - name: ExistingPrivateSubnet3
     availabilityZone: {{ region }}c
     # todo [customize] per account & region
     id: "{{ private_subnet_zone_c }}"

#     # Exported output's name from another stack
#     # Only specify either id or idFromStackOutput but not both
#     #idFromStackOutput: myinfra-PublicSubnet1
#
#   #
#   # Advanced: Unmanaged/existing private subnet reused but not managed by kube-aws
#   #
#   # A nat gateway(ngw) and a route table contains the route to the ngw must have been properly configured by YOU.
#   # kube-aws tries to reuse the subnet specified by id or idFromStackOutput but kube-aws never modify the subnet
#   #
#   - name: ExistingPrivateSubnet1
#     # Beware that `availabilityZone` can't be omitted; it must be the one in which the subnet exists.
#     availabilityZone: us-west-1a
#     # Existing subnet.
#     id: "subnet-xxxxxxxx"
#     # Exported output's name from another stack
#     # Only specify either id or idFromStackOutput but not both
#     #idFromStackOutput: myinfra-PrivateSubnet1
#
#   #
#   # Advanced: Managed private subnet with an existing NAT gateway
#   #
#   # kube-aws tries to reuse the ngw specified by id or idFromStackOutput
#   # by adding a route to the ngw to a route table managed by kube-aws
#   #
#   # Please be sure that the NGW is properly deployed. kube-aws will never modify ngw itself.
#   #
#   - name: ManagedPrivateSubnetWithExistingNGW
#     private: true
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#     natGateway:
#       id: "ngw-xxxxxxxx"
#       # Exported output's name from another stack
#       # Only specify either id or idFromStackOutput but not both
#       #idFromStackOutput: myinfra-PrivateSubnet1
#
#   #
#   # Advanced: Managed private subnet with an existing NAT gateway
#   #
#   # kube-aws tries to reuse the ngw specified by id or idFromStackOutput
#   # by adding a route to the ngw to a route table managed by kube-aws
#   #
#   # Please be sure that the NGW is properly deployed. kube-aws will never modify ngw itself.
#   # For example, kube-aws won't assign a pre-allocated EIP to the existing ngw for you.
#   #
#   - name: ManagedPrivateSubnetWithExistingNGW
#     private: true
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#     natGateway:
#       # Pre-allocated NAT Gateway. Used with private subnets.
#       id: "ngw-xxxxxxxx"
#       # Exported output's name from another stack
#       # Only specify either id or idFromStackOutput but not both
#       #idFromStackOutput: myinfra-PrivateSubnet1
#
#   #
#   # Advanced: Managed private subnet with an existing EIP for kube-aws managed NGW
#   #
#   # kube-aws tries to reuse the EIP specified by eipAllocationId
#   # by associating the EIP to a NGW managed by kube-aws.
#   # Please be sure that kube-aws won't assign an EIP to an existing NGW i.e.
#   # either natGateway.id or eipAllocationId can be specified but not both.
#   #
#   - name: ManagedPrivateSubnetWithManagedNGWWithExistingEIP
#     private: true
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#     natGateway:
#       # Pre-allocated EIP for NAT Gateways. Used with private subnets.
#       eipAllocationId: eipalloc-xxxxxxxx
#
#   #
#   # Advanced: Managed private subnet with an existing route table
#   #
#   # kube-aws tries to reuse the route table specified by id or idFromStackOutput
#   # by assigning this subnet to the route table.
#   #
#   # Please be sure that it's your responsibility to:
#   # * Configure an AWS managed NAT or a NAT instance or an another NAT and
#   # * Add a route to the NAT to the route table being reused
#   #
#   # i.e. kube-aws neither modify route table nor create other related resources like
#   # ngw, route to nat gateway, eip for ngw, etc.
#   #
#   - name: ManagedPrivateSubnetWithExistingRouteTable
#     private: true
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#     routeTable:
#       # Pre-allocated route table
#       id: "rtb-xxxxxxxx"
#       # Exported output's name from another stack
#       # Only specify either id or idFromStackOutput but not both
#       #idFromStackOutput: myinfra-PrivateRouteTable1
#
#   #
#   # Advanced: Managed public subnet with an existing route table
#   #
#   # kube-aws tries to reuse the route table specified by id or idFromStackOutput
#   # by assigning this subnet to the route table.
#   #
#   # Please be sure that it's your responsibility to:
#   # * Configure an internet gateway(IGW) and
#   # * Attach the IGW to the VPC you're deploying to
#   # * Add a route to the IGW to the route table being reused
#   #
#   # i.e. kube-aws neither modify route table nor create other related resources like
#   # igw, route to igw, igw vpc attachment, etc.
#   #
#   - name: ManagedPublicSubnetWithExistingRouteTable
#     availabilityZone: us-west-1a
#     instanceCIDR: "10.0.1.0/24"
#     routeTable:
#       # Pre-allocated route table
#       id: "rtb-xxxxxxxx"
#       # Exported output's name from another stack
#       # Only specify either id or idFromStackOutput but not both
#       #idFromStackOutput: myinfra-PublicRouteTable1

# Kubernetes Network CIDRs
# You can change your serviceCIDR or podCIDR and kube-aws will facilitate the migration
# see https://github.com/kubernetes-incubator/kube-aws/issues/1307
# NOTE: requires cluster downtime.

# CIDR for all service IP addresses
# serviceCIDR: "10.3.0.0/24"

# CIDR for all pod IP addresses
# Typically set to 10.244.0.0/16 for Flannel or Canal and 192.168.0.0/16 for Calico
# podCIDR: "10.2.0.0/16"

# IP address of Kubernetes dns service (must be contained by serviceCIDR)
# dnsServiceIP: 10.3.0.10

# Uncomment to provision nodes without a public IP. This assumes your VPC route table is setup to route to the internet via a NAT gateway.
# If you did not set vpcId and routeTableId the cluster will not bootstrap.
# mapPublicIPs: false

# Expiration in days from creation time of TLS assets. By default, the CA will
# expire in 10 years and the server and client certificates will expire in 1
# year.
#tlsCADurationDays: 3650
#tlsCertDurationDays: 365

# Use custom images for kube-aws  and  kubernetes  components. Especially if you are deploying in cn-north-1 where gcr.io is blocked
# and pulling from quay or dockerhub is slow and you get many timeouts.

# Version of hyperkube image to use. This is the tag for the hyperkube image repository.
kubernetesVersion: {{ k8s_version }}

# Hyperkube image repository to use.
# hyperkubeImage:
#   repo: k8s.gcr.io/hyperkube-amd64
#   rktPullDocker: true

# AWS CLI image repository to use.
# awsCliImage:
#   repo: quay.io/coreos/awscli
#   tag: master
#   rktPullDocker: false

# Cluster Autoscaler image repository to use.
#clusterAutoscalerImage:
#  repo: k8s.gcr.io/cluster-autoscaler
#  tag: v1.1.0
#  rktPullDocker: false

# Cluster Proportional Autoscaler image repository to use.
#clusterProportionalAutoscalerImage:
#  repo: k8s.gcr.io/cluster-proportional-autoscaler-amd64
#  tag: 1.1.2
#  rktPullDocker: false

# kube2iam image repository to use.
#kube2iamImage:
#  repo: jtblin/kube2iam
#  tag: 0.9.0
#  rktPullDocker: false

# kube DNS image repository to use.
#kubeDnsImage:
#  repo: k8s.gcr.io/k8s-dns-kube-dns-amd64
#  tag: 1.14.7
#  rktPullDocker: false

# kube DNS Masq image repository to use.
#kubeDnsMasqImage:
#  repo: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64
#  tag: 1.14.7
#  rktPullDocker: false

# Core DNS image repository to use.
#coreDnsImage:
#  repo: coredns/coredns
#  tag: 1.1.3
#  rktPullDocker: false

# kube rescheduler image repository to use.
#kubeReschedulerImage:
#  repo: k8s.gcr.io/rescheduler-amd64
#  tag: v0.3.2
#  rktPullDocker: false

# DNS Masq metrics image repository to use.
#dnsMasqMetricsImage:
#  repo: k8s.gcr.io/k8s-dns-sidecar-amd64
#  tag: 1.14.7
#  rktPullDocker: false

# Exec Healthz image repository to use.
#execHealthzImage:
#  repo: k8s.gcr.io/exechealthz-amd64
#  tag: 1.2
#  rktPullDocker: false

# Helm image repository to use.
#helmImage:
#  repo: quay.io/kube-aws/helm
#  tag: v2.6.0
#  rktPullDocker: false

# Tiller (Helm backend) image repository to use.
tillerImage:
  repo: gcr.io/kubernetes-helm/tiller
  tag: v2.9.1
  rktPullDocker: false

# Heapster image repository to use.
#heapsterImage:
#  repo: k8s.gcr.io/heapster
#  tag: v1.5.0
#  rktPullDocker: false

# Metrics Server image repository to use.
#metricsServerImage:
#  repo: k8s.gcr.io/metrics-server-amd64
#  tag: v0.2.1
#  rktPullDocker: false

# Addon Resizer image repository to use.
#addonResizerImage:
#  repo: k8s.gcr.io/addon-resizer
#  tag: 1.8.1
#  rktPullDocker: false

# Kube Dashboard image repository to use.
#kubernetesDashboardImage:
#  repo: k8s.gcr.io/kubernetes-dashboard-amd64
#  tag: v1.10.1
#  rktPullDocker: false

# Pause image repository to use.This works only if you are deploying your cluster in "cn-north-1" region.
#pauseImage:
#  repo: k8s.gcr.io/pause-amd64
#  tag: 3.1
#  rktPullDocker: false

# JournaldCloudWatchLogsImage image repository to use. This sends journald logs to CloudWatch.
#journaldCloudWatchLogsImage:
#  repo: "jollinshead/journald-cloudwatch-logs"
#  tag: "0.1"
#  rktPullDocker: true

kubernetes:
  # If enabled, instructs the controller manager to automatically issue TLS certificates to worker nodes via
  # certificate signing requests (csr) made to the API server using the bootstrap token. It's recommended to
  # also enable the rbac plugin in order to limit requests using the bootstrap token to only be able to make
  # requests related to certificate provisioning.
  # The bootstrap token is automatically generated in ./credentials/kubelet-tls-bootstrap-token.
  encryptionAtRest:
    enabled: false

#  controllerManager:
#    resources:
#      requests:
#        cpu: 100m
#        memory: 100M
#      limits:
#        cpu: 250m
#        memory: 512M

# Kubernetes Self-hosted networking daemonsets
# Choose either 'canal' (calico+flannel) or 'flannel'
# (choose 'canal' if you require calico or kubernetes NetworkPolicy firewalling).
# Enable Typha if you have 50+ nodes in your cluster. Without Typha, the load on the API server and Felix’s CPU usage
# increases substantially as the number of nodes is increased.
  networking:
    selfHosting:
      type: flannel      # either "canal" or "flannel"
      typha: false     # enable for type 'canal' for 50+ node clusters
#      calicoNodeImage:
#        repo: quay.io/calico/node
#        tag: v3.1.3
#      calicoCniImage:
#        repo: quay.io/calico/cni
#        tag: v3.1.3
#      flannelImage:
#        repo: quay.io/coreos/flannel
#        tag: v0.9.1
#      flannelCniImage:
#        repo: quay.io/coreos/flannel-cni
#        tag: v0.3.0
#     typhaImage:
#        repo: quay.io/calico/typha
#        tag: v0.7.4

# Create MountTargets to subnets managed by kube-aws for a pre-existing Elastic File System (Amazon EFS),
# and then mount to every node.
#
# This is for mounting an EFS to every node.
# If you'd like to mount an EFS only to a specific node pool, set `worker.nodePools[].elasticFileSystemId` instead.
#
# Enter the resource id, eg "fs-47a2c22e"
# This is a NFS share that will be available across the entire cluster through a hostPath volume on the "/efs" mountpoint
#
# You can create a new EFS volume using the CLI:
# $ aws efs create-file-system --creation-token $(uuidgen)
#
# Beware that:
# * an EFS file system can not be mounted from multiple subnets from the same availability zone and
#   therefore this feature won't work like you might have expected when you're deploying your cluster to an existing VPC and
#   wanted to mount an existing EFS to the subnet(s) created by kube-aws
#   See https://github.com/kubernetes-incubator/kube-aws/issues/208 for more information
# * kube-aws doesn't create MountTargets for existing subnets(=NOT managed by kube-aws). If you'd like to bring your own subnets
#   to be used by kube-aws, it is now your responsibility to configure the subnets, including creating MountTargets, accordingly
#   to your requirements
#elasticFileSystemId: fs-47a2c22e

# Create shared persistent volume
#sharedPersistentVolume: false

# Determines the container runtime for kubernetes to use. Accepts 'docker' or 'rkt'.
containerRuntime: docker

# If you do not want kube-aws to manage certificaes, set it to false. If you do that
# you are responsible for making sure that nodes have correct certificates by the time
# daemons start up.
manageCertificates: true

# When enabled, autoscaling groups managing controller nodes wait for nodes to be up and running.
# It is enabled by default.
#waitSignal:
#  enabled: true
#   maxBatchSize: 1

# Autosaves all Kubernetes resources (in .json format) to a bucket 's3:.../<your-cluster-name>/backup/*'.
# The autosave process executes on start-up and repeats every 24 hours.
kubeResourcesAutosave:
  enabled: true
#

# Set kube-system namespace labels:
# In order to target a namespace for network policies the namepace needs to be labeled.
# Feel free to remove or alter this setting to change the labels for the kube-system namespace.
# Values need to be a dictionary of values
# foo: bar
# bar: foo
kubeSystemNamespaceLabels:
  name: kube-system

# Accessing Dashboard:
# By default, the kubernetes dashboard has admin privileges and doesn't require authentication.
# Set 'adminPrivileges: false' to restrict access using a token or kubeconfig file.
# An example of how to generate a token is provided in the documentation: https://kubernetes-incubator.github.io/kube-aws/advanced-topics/kubernetes-dashboard.html

# Access the dashboard  using 'kubectl proxy:
# insecureLogin: false ==> "http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/"
# insecureLogin: true ==>  "http://localhost:8001/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/"
# enabled: true
# Expose the dashboard
# Please check the documentation: https://kubernetes-incubator.github.io/kube-aws/advanced-topics/kubernetes-dashboard.html

kubernetesDashboard:
  adminPrivileges: true
  insecureLogin: false
  allowSkipLogin: false # Only set to true when using dashboard image version v1.10.1+
  replicas: 1 # 1 is the default
  enabled: false
# # Optional resource change for Dashboard can be done via using the resources block below and changing the values.
# # Values below are the default already if not set.
# resources:
#   requests:
#     cpu: 100m
#     memory: 100Mi
#   limits:
#     cpu: 100m
#     memory: 100Mi

# When enabled, all nodes will forward journald logs to AWS CloudWatch.
# It is disabled by default.
#cloudWatchLogging:
# enabled: false
# retentionInDays: 7
# # When enabled, feedback from Journald logs (with an applied filter) will be outputted during kube-aws 'apply | up'.
# # It is enabled by default - provided cloudWatchLogging is enabled.
# localStreaming:
#  enabled: true
#  filter:  `{ $.priority = "CRIT" || $.priority = "WARNING" && $.transport = "journal" && $.systemdUnit = "init.scope" }`
#  interval: 60

# When enabled, all nodes will have Amazon SSM Agent installed.
# It is disabled by default.
# You might want to set 'managedPolicies' up to run the agents properly. More details: http://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-configuring-access-policies.html
#amazonSsmAgent:
#  enabled: true
#  downloadUrl: https://github.com/DailyHotel/amazon-ssm-agent/releases/download/v2.0.805.1/ssm.linux-amd64.tar.gz
#  sha1sum: a6fff8f9839d5905934e7e3df3123b54020a1f5e


# KubeDNS
kubeDns:
  # Define which DNS provider to use (kube-dns or coredns), default kube-dns.
  provider: kube-dns

# When enabled, will enable a DNS-masq DaemonSet to make PODs to resolve DNS names via locally running dnsmasq
# It is disabled by default.
# nodeLocalResolver: false
  # Extra DnsMasq options to use when running the nodeLocalResolver
  # nodeLocalResolverOptions:
  # - --neg-ttl=10
  # - --no-ping

# When enabled, will deploy kube-dns to K8s controllers instead of workers.
# deployToControllers: false

  # DNS Autoscaler
  # Ref: https://github.com/kubernetes-incubator/cluster-proportional-autoscaler/
  autoscaler:
    coresPerReplica: 256
    nodesPerReplica: 16
    min: 2

kubeProxy:
  # Use IPVS kube-proxy mode instead of [default] iptables one (requires Kubernetes 1.9.0+ to work reliably)
  # This is intended to address performance issues of iptables mode for clusters with big number of nodes and services
  # FIXME For those who use hyperkube version 'v1.9.0' / 'v1.9.0_coreos.0', your image may lack `ipset` utility
  # FIXME Please see: https://github.com/kubernetes/kubernetes/issues/57321 (next Kubernetes release will have a fix)
  # FIXME https://github.com/kubernetes/kubernetes/commit/787a55bb67ccd2da14aa6e7f91289c859beecb5f#diff-bf0f8d724d18f700f3c821aa5a74f4cf
  # FIXME IPVS integration is still green, proceed with care! You may get fixed hyperkube image from 'ivanilves/hyperkube' Docker repo
  ipvsMode:
    enabled: false
    scheduler: rr
    syncPeriod: 300s
    minSyncPeriod: 60s

# When enabled, CloudFormation events will stream to stdout during kube-aws 'update | up'.
# It is enabled by default.
#cloudFormationStreaming: true

# Addon features
addons:
  # Will create a cluster-autoscaler (CA) deployment in the cluster.
  # CA runs only on controller nodes by default, to turn this off set `experimental.clusterAutoscalerSupport.enabled` to false.
  # If you want to run CA on worker nodes, turn on `worker.nodePools[].clusterAutoscalerSupport.enabled` for the node pool.
  clusterAutoscaler:
    enabled: false
    # Options below can be used to inject custom settings for the autoscaler.
    # Sensible defaults are already configured in the controller-cloud-config but if you wish to override them simply
    # add them here and they'll take precedence.
    #options:
    #  flag-name: value
    #  v: 5
    #  expander: least-waste

  # When enabled, Kubernetes rescheduler is deployed to the cluster controller(s)
  # This feature is experimental currently so may not be production ready
  rescheduler:
    enabled: false

  # Metrics Server (https://github.com/kubernetes-incubator/metrics-server)
  metricsServer:
    enabled: true

  # When set to true this configures security groups for prometheus between nodes.
  # This includes the following ports: 10252, 10251, 10250, 9100, and 4194
  prometheus:
    securityGroupsEnabled: true

# Experimental features will change in backward-incompatible ways
experimental:
  # Enable admission controllers
  admission:
    podSecurityPolicy:
      enabled: false
    # alwaysPullImages Note
    # Recommended to turn this on when and only when docker registries are reliable enough.
    # Otherwise, automatic pod restarts can be delayed due to temporally docker registry outages.
    # Please see https://github.com/kubernetes-incubator/kube-aws/pull/1009#discussion_r151197787 for more info.
    alwaysPullImages:
      enabled: false
    denyEscalatingExec:
      enabled: false
    initializers:
      enabled: false
    # Priority enables PodPriority in the API server, scheduler and kubelet. you need to manually add the
    # featureGate PodPriority:true in the worker
    priority:
      enabled: false
    mutatingAdmissionWebhook:
      enabled: false
    validatingAdmissionWebhook:
      enabled: false
    OwnerReferencesPermissionEnforcement:
      enabled: false
    # persistentVolumeClaimResize enables PersistentVolumeClaim in the API server, scheduler and kubelet. you need to manually add the
    # featureGate ExpandPersistentVolumes:true in the worker
    persistentVolumeClaimResize:
      enabled: false

  # Used to provide `/etc/environment` env vars with values from arbitrary CloudFormation refs
  awsEnvironment:
    enabled: false
    environment:
      CFNSTACK: '{ "Ref" : "AWS::StackId" }'

  # Enable audit log for apiserver. Recommended when `rbac` is enabled.
  auditLog:
    enabled: true
    maxage: 1
    logpath: "-"
    maxBackup: 1
    maxSize: 100

  # See https://kubernetes.io/docs/admin/authentication/#webhook-token-authentication for more information
  authentication:
    webhook:
      enabled: false
      cacheTTL: 1m0s
      configBase64: base64-encoded-webhook-yaml

  # Add predefined set of labels to the controller nodes
  # The set includes names of launch configurations and autoscaling groups
  awsNodeLabels:
    enabled: false

  # Provision controller nodes with IAM permissions and node labels to run cluster-autoscaler (assuming `addons.clusterAutoscaler.enabled` is true)
  clusterAutoscalerSupport:
    enabled: false

  # If enabled, instructs the controller manager to automatically issue TLS certificates to worker nodes via
  # certificate signing requests (csr) made to the API server using the bootstrap token. It's recommended to
  # also enable the rbac plugin in order to limit requests using the bootstrap token to only be able to make
  # requests related to certificate provisioning.
  # The bootstrap token is automatically generated in ./credentials/kubelet-tls-bootstrap-token.
  tlsBootstrap:
    enabled: false

  # Enables the Node authorization + node restriction admission controller (requires Kubernetes 1.7.4+)
  # Requires TLS bootstrapping to be enabled as well
  nodeAuthorizer:
    enabled: false

  # This option has not yet been tested with rkt as container runtime
  ephemeralImageStorage:
    enabled: false

  # When enabled this will install the kiam daemon set using the repo from `kiamImage`
  # It will also grant sts:assumeRole permission to the IAM role for controller nodes (only controller nodes where the kiam server is hosted need this, worker nodes do not)
  # This will use SSL certificates generated during `kube-aws render credentials`, ensure this has been run with kube-aws v0.9.9+
  # This is intended to be used in combination with .controller.iam.role.name. See #297 for more information.
  kiamSupport:
    enabled: true
    image:
      repo: quay.io/uswitch/kiam
      tag: v2.8
      rktPullDocker: false
    sessionDuration: 15m
    serverAddresses:
      serverAddress: localhost:443
      agentAddress: kiam-server:443
    # Optional resource change for kiam servers/agents can be done via using the resources block below and changing the values.
    # Values below are the default if not set.
      serverResources:
        requests:
          cpu: 10m
          memory: 60Mi
        limits:
          cpu: 40m
          memory: 100Mi
      agentResources:
        requests:
          cpu: 5m
          memory: 30Mi
        limits:
          cpu: 20m
          memory: 60Mi


  # When enabled this will install the kube2iam daemon set using the repo from `kube2iamImage`
  # It will also grant sts:assumeRole permission to the IAM role for controller nodes.
  # This is intended to be used in combination with .controller.iam.role.name. See #297 for more information.
  kube2IamSupport:
    enabled: false

  # When enabled, `kubectl drain` is run when the instance is being replaced by the auto scaling group, or when
  # the instance receives a termination notice (in case of spot instances)
  nodeDrainer:
    enabled: true
    # Maximum time to wait, in minutes, for the node to be completely drained. Must be an integer between 1 and 60.
    drainTimeout: 5
    # IAM role to assume with kube2iam for the pod in "kube-node-drainer-asg-status-updater" deployment.
    iamRole:
      # Empty, inactive by default. Set it to valid ARN "arn: arn:aws:iam::0123456789012:role/roleName" to activate.
      arn: ""

  # Configure OpenID Connect token authenticator plugin in Kubernetes API server.
  # For using Dex as a custom OIDC provider, please check "contrib/dex/README.md".
  # WARNING: always use "https" for "issuerUrl", otherwise the Kubernetes API server will not start correctly.
  oidc:
    enabled: true
    issuerUrl: {{ oidc_issuer_url }}
    clientId: "kubectl-login"
    usernameClaim: "name"
    groupsClaim: "groups"

  # When set to true this configures the k8s aws provider so that it doesn't modify every node's security group
  # to include an additional ingress rule per an ELB created for a k8s service whose type is "LoadBalancer".
  # It requires that the user has setup a rule that allows inbound traffic on kubelet ports
  # from the local VPC subnet so that load balancers can access it.  Ref: https://github.com/kubernetes/kubernetes/issues/26670
  disableSecurityGroupIngress: false

  # Command line flag passed to the controller-manager. (default 40s)
  # This is the amount of time which we allow running Node to be unresponsive before marking it unhealthy.
  # Must be N times more than kubelet's nodeStatusUpdateFrequency (default 10s).
  nodeMonitorGracePeriod: "60s"

kubelet:
  # Tell Kubelet to renew its certificate as its expiration time is approaching
  # Requires Experimental.tlsBootstrap to be enabled
  RotateCerts:
    enabled: false
  # Currently CPU, memory and storage are supported.
  # How resources are reserved: https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/
  # How to size these limits: https://kubernetes.io/blog/2016/11/visualize-kubelet-performance-with-node-dashboard/
  # kubeReserved is used to reserve capacity for kubernetes system daemons
  #kubeReserved: "cpu=100m,memory=100Mi,ephemeral-storage=1Gi"
  # systemReserved is used to reserve capacity for OS system daemons
  #systemReserved: "cpu=100m,memory=100Mi,ephemeral-storage=1Gi"

# AWS Tags for cloudformation stack resources
stackTags:
  systemCode: {{ platform }}
  environment: {{ environment_type }}
  coco-environment-tag: {{ stack_name }}
  name: {{ stack_name }}
{% for tag in tags %}
  {{ tag.name }}: {{ tag.value }}
{% endfor %}

# User-provided YAML map available in control-plane's stack-template.json
#customSettings:
#  key1: [ 1, 2, 3 ]

# HostOS - specific configurations relating to host operating system
hostOS:

# prompt - allows you to customise your shell prompt settings and colours
# exmaple all on with defaults: -
# server-label|cluster-name core@hostname pwd $
# server-label|cluster-name hostname pwd #

# valid colours are :-
# default-colour, black, red, green, yellow, blue, magenta, cyan, white, dark-gray
# light-red, light-green, light-yellow, light-blue, light-magenta, light-cyan, light-white

#  bashPrompt:
#    enabled: true                          # Turn the customised shell prompts off.
#    include-pwd: true                      # Include the present directory in each prompt.
#    include-hostname: true                 # Include the hostname in each prompt.
#    include-user: true                     # Include the username in each prompt.
#    cluster-colour: light-cyan
#    divider: "|"                           # Divider between server type and cluster name, e.g. etcd|mycluster
#    divider-colour: default-colour
#    etcd-label: etcd                       # Start prompt 'etcd' on prompts for etcd servers.
#    etcd-colour: light-green
#    controller-label: master               # Start prompt with 'master' on controllers.
#    controller-colour: light-red
#    worker-label: node                     # Start prompt with 'node' on workers.
#    worker-colour: light-blue
#    root-user-colour: light-red            # Colour of the latter half of the prompt changes for root.
#    non-root-user-colour: light-green      # All other users will see this colour.
#    directory-colour: light-blue           # Colour of the directory indicator and last prompt symbol ($ or #)

# motdBanner - display etcd, kubernetes and kube-aws versions in the login message-of-the-day banner.
#  motdBanner:
#    enabled: true
#    etcd-colour: light-green
#    kubernetes-colour: light-blue
#    kube-aws-colour: light-blue

